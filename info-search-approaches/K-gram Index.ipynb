{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-gram Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Made By Yuliia Hetman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words : 93799\n",
      "Total number of tokens : 93799\n",
      "\n",
      " ['$відвернуло$', '$полковницький$', '$вібравши$'] \n",
      "\n",
      "CPU times: user 993 ms, sys: 290 ms, total: 1.28 s\n",
      "Wall time: 1.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from multiprocessing import Pool\n",
    "import re, os\n",
    "from string import digits\n",
    "\n",
    "\n",
    "docIDs = list(range(0, 18))\n",
    "\n",
    "\n",
    "def isUkrainian(word):\n",
    "    ukrainian = [96, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1100, 1102, 1103, 1108, 1110, 1111, 1169, 8217]\n",
    "    for l in word:\n",
    "        if ord(l) not in ukrainian:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def check_vocabulary(vocabulary):\n",
    "        return ['$' + k + '$' for k in set(vocabulary) if isUkrainian(k) == True]\n",
    "        \n",
    "def delete_sings_digits(content):\n",
    "    punc = '''!()-'”№[]{};:\"\\,“«»<>./?@#$%—…^&*_~|–abcdefghijklmnoqprstuvwxyz'''\n",
    "    content = \"\".join([ele for ele in content.lower() if ele not in punc])\n",
    "    table = str.maketrans('', '', digits)\n",
    "    content = content.translate(table)\n",
    "    return content\n",
    "\n",
    "\n",
    "def create_vocabulary(content):    \n",
    "    raw_content = delete_sings_digits(content)\n",
    "    vocab = [i.lower() for i in raw_content.split() if i != '']\n",
    "    return vocab\n",
    "\n",
    "def read_book(book):\n",
    "    dirname='../books/'\n",
    "    with open(dirname + book, 'r', encoding='utf-8') as file:\n",
    "        text = create_vocabulary(file.read())\n",
    "    return text\n",
    "\n",
    "        \n",
    "def get_vocabulary(dirname='../books/'):\n",
    "    books = os.listdir(dirname)\n",
    "    vocabulary = list()\n",
    "    dictionary = dict()\n",
    "    memory = 0\n",
    "    with Pool(8) as p:\n",
    "        vocabul = p.map(read_book, books)\n",
    "    for i, book in enumerate(books):\n",
    "        dictionary[book.replace('.txt', '')] = vocabul[i]\n",
    "        vocabulary += vocabul[i]\n",
    "        memory += os.stat(dirname + book).st_size\n",
    "    return memory, vocabulary, dictionary\n",
    "\n",
    "\n",
    "memory, vocabulary, dictionary = get_vocabulary()\n",
    "vocabulary = check_vocabulary(vocabulary)\n",
    "print(f'Total number of words : {len(vocabulary)}')\n",
    "print(f'Total number of tokens : {len(set(vocabulary))}')\n",
    "print('\\n', vocabulary[:3],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 µs, sys: 0 ns, total: 28 µs\n",
      "Wall time: 31.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def split_word_into_k_grams(k, word):\n",
    "    return [(word[i:i + k], word) for i in range(len(word) - k + 1)]\n",
    "\n",
    "def parallelize_splitting(word):\n",
    "    k_grams = list()\n",
    "    K = [2, 3, 4]\n",
    "    for k in K:\n",
    "        k_grams += split_word_into_k_grams(k, word)\n",
    "    return k_grams\n",
    "\n",
    "\n",
    "class K_Grams():\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.final_grams = dict()\n",
    "        self.grams = self.split_all_words()\n",
    "        self.k_grams = list()\n",
    "        self.max_k = 4\n",
    "    \n",
    "    def split_all_words(self):\n",
    "        with Pool(10) as p:\n",
    "            self.k_grams = p.map(parallelize_splitting, self.vocabulary)\n",
    "        for word in self.k_grams:\n",
    "            for (gram, w) in word:\n",
    "                if (gram in self.final_grams):\n",
    "                    self.final_grams[gram] += [w]\n",
    "                else:\n",
    "                    self.final_grams[gram] = [w]\n",
    "        return self.final_grams\n",
    "    \n",
    "\n",
    "    def find(self, string):\n",
    "        if len(string) > self.max_k:\n",
    "            return self.grams[string[:max_k]]\n",
    "        else:\n",
    "            return self.grams[string]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of how grams are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND(word1, word2):\n",
    "    return list(set(word1) & set(word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def processing_boolean_req(grams, string, k=3):\n",
    "    s = '$' + string + '$'\n",
    "    jocker = string.find('*')\n",
    "    if jocker == -1:\n",
    "        return grams.find(string[:k])\n",
    "    parts = string.split('*')\n",
    "    l1 = grams.find(parts[0])\n",
    "    for i in range(1, len(parts)):\n",
    "        l1 = AND(l1, grams.find(parts[i][:k]))\n",
    "    return l1\n",
    "\n",
    "\n",
    "def compare_particles(term, parts, temp=0):\n",
    "    for i in range(len(parts)):\n",
    "        ind = term.find(parts[i])\n",
    "        if temp < ind:\n",
    "            temp = ind\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def post_filtering(results, template, k=3):\n",
    "    satisfactory = list()\n",
    "    start = '$' + template[:k - 1]\n",
    "    end = template[1 - k :] + '$'\n",
    "    parts = template.split('*')\n",
    "    for word in results:\n",
    "        if word[:k] == start and word[-k :] == end and compare_particles(word, parts) == True:\n",
    "            satisfactory.append(word)\n",
    "        else:\n",
    "            continue\n",
    "    return satisfactory\n",
    "\n",
    "\n",
    "def beautiful_head_print(words):\n",
    "    for word in words[:5]:\n",
    "        print(word[1:-1])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$покатівськи$', '$понімецьки$', '$покритки$', '$потихеньки$', '$посусідськи$', '$політки$', '$похапки$', '$полумиски$', '$поволеньки$', '$поміщики$', '$подорожники$', '$позапорозьки$', '$посмішки$', '$повітки$', '$похристиянськи$', '$поприятельськи$', '$подруженьки$', '$пописьменськи$', '$порошки$', '$посторонки$', '$починки$', '$полковники$', '$познавецьки$', '$полянки$', '$подушки$', '$полонинки$', '$поцілунки$', '$політики$', '$помолодецьки$', '$полички$', '$полюдськи$', '$полядськи$', '$подножки$', '$подалисятаки$', '$потоки$', '$посиденьки$', '$поголоски$', '$постаросвітськи$', '$польки$', '$поговірки$', '$початки$', '$поразки$', '$поазіатськи$', '$покоролівськи$', '$поклики$', '$помосковськи$', '$поварварськи$', '$понаськи$', '$подалеки$', '$поворозки$', '$попанцьки$', '$повозки$', '$попаски$', '$покрадьки$', '$потайники$', '$половики$', '$повіки$', '$покоївки$', '$потемки$', '$поминки$', '$попанськи$', '$помилки$', '$порицарськи$', '$подарунки$', '$побиванки$', '$поясники$', '$посвійськи$', '$пошепки$', '$порядки$', '$попідруки$', '$полки$', '$податки$', '$потомки$', '$позички$', '$позики$', '$поки$', '$поправки$', '$подвижники$', '$порадоньки$', '$пошляхетськи$', '$полотенщики$', '$помонгольськи$', '$поведінки$', '$поримськи$', '$поборники$', '$поступки$', '$похазяйськи$', '$помоглотаки$', '$посільськи$', '$потайки$', '$покидавтаки$', '$поляки$', '$померки$', '$поетики$', '$походеньки$', '$поліняки$', '$подворянськи$', '$попредківськи$', '$погетьманськи$', '$побратерськи$', '$помічники$', '$полудрабки$', '$політехніки$', '$подяки$', '$полицарськи$', '$повідки$', '$посвідки$', '$побатьківськи$', '$повіточки$', '$помощники$', '$покозацьки$', '$потурецьки$']\n",
      "CPU times: user 2.74 s, sys: 434 ms, total: 3.17 s\n",
      "Wall time: 3.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "template = 'по*ки'\n",
    "grams = K_Grams(vocabulary)\n",
    "result_bool_request = processing_boolean_req(grams, template)\n",
    "checked_results = post_filtering(result_bool_request, template)\n",
    "print(checked_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "покатівськи\n",
      "понімецьки\n",
      "покритки\n",
      "потихеньки\n",
      "посусідськи\n",
      "\n",
      "\n",
      "CPU times: user 534 µs, sys: 0 ns, total: 534 µs\n",
      "Wall time: 320 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "beautiful_head_print(checked_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
