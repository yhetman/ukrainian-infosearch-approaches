{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPIMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Made by Yuliia Hetman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from multiprocessing import Pool\n",
    "import re, os\n",
    "from string import digits\n",
    "\n",
    "docIDs = list(range(0, 18))\n",
    "\n",
    "\n",
    "def isUkrainian(word):\n",
    "    ukrainian = [96, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1100, 1102, 1103, 1108, 1110, 1111, 1169, 8217]\n",
    "    for l in word:\n",
    "        if ord(l) not in ukrainian:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def check_vocabulary(vocabulary):\n",
    "        return [k + '$' for k in set(vocabulary) if isUkrainian(k) == True]\n",
    "        \n",
    "def delete_sings_digits(content):\n",
    "    punc = '''!()-'”№[]{};:\"\\,“«»<>./?@#$%—…^&*_~|–abcdefghijklmnoqprstuvwxyz'''\n",
    "    content = \"\".join([ele for ele in content.lower() if ele not in punc])\n",
    "    table = str.maketrans('', '', digits)\n",
    "    content = content.translate(table)\n",
    "    return content\n",
    "\n",
    "\n",
    "def create_vocabulary(content):    \n",
    "    raw_content = delete_sings_digits(content)\n",
    "    vocab = [i.lower() for i in raw_content.split() if i != '']\n",
    "    return vocab\n",
    "\n",
    "def read_book(book):\n",
    "    dirname='../books/'\n",
    "    with open(dirname + book, 'r', encoding='utf-8') as file:\n",
    "        text = create_vocabulary(file.read())\n",
    "    return text\n",
    "\n",
    "        \n",
    "def get_vocabulary(dirname='../books/'):\n",
    "    books = os.listdir(dirname)\n",
    "    vocabulary = list()\n",
    "    dictionary = dict()\n",
    "    memory = 0\n",
    "    with Pool(8) as p:\n",
    "        vocabul = p.map(read_book, books)\n",
    "    for i, book in enumerate(books):\n",
    "        dictionary[book.replace('.txt', '')] = vocabul[i]\n",
    "        vocabulary += vocabul[i]\n",
    "        memory += os.stat(dirname + book).st_size\n",
    "    return memory, vocabulary, dictionary\n",
    "\n",
    "\n",
    "memory, vocabulary, dictionary = get_vocabulary()\n",
    "print(f'Total number of words : {len(vocabulary)}')\n",
    "print(f'Total number of tokens : {len(set(vocabulary))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--path PATH] [--block_size BLOCK_SIZE]\n",
      "                             [--output OUTPUT] [--filename FILENAME]\n",
      "                             [--compression COMPRESSION]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/yhetman/.local/share/jupyter/runtime/kernel-0152529b-5941-4243-9384-b2f5a0f333a1.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yhetman/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import nltk\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "\n",
    "class SPIMI(object):\n",
    "    def __init__(self, output_dir):\n",
    "        self.output_dir = output_dir\n",
    "        self.overhead_parameter = 10000 * 512\n",
    "        self.block_files = []\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.mkdir(self.output_dir)\n",
    "\n",
    "\n",
    "    def directory_listing(self, root):\n",
    "        files_in_dir = os.listdir(root)\n",
    "        files_in_dir = [os.path.abspath(os.path.join(root, file)) for file in files_in_dir]\n",
    "        return files_in_dir\n",
    "\n",
    "    def file_reading(self, filename):\n",
    "        with open(filename, 'r', encoding='utf8') as f:\n",
    "            file_content = f.read()\n",
    "        return file_content\n",
    "\n",
    "    def tokenizer(self, file_content):\n",
    "        return [token.lower() for token in file_content.split() if token != '']\n",
    "\n",
    "    def linguistic_transform(self, file_content):\n",
    "        punc = '''!()-'”№[]{};:\"\\,“«»<>./?@#$%—…^&*_~|–abcdefghijklmnoqprstuvwxyz'''\n",
    "        content = \"\".join([char for char in file_content.lower() if char not in file_content])\n",
    "        table = str.maketrans('', '', string.digits)\n",
    "        content = content.translate(table)\n",
    "        return content\n",
    "    \n",
    "    \n",
    "#         stemmer = PorterStemmer()\n",
    "#         inclusion = list(string.ascii_lowercase) + list(string.ascii_uppercase)\n",
    "#         stop_words = list(nltk.corpus.stopwords.words('english'))\n",
    "#         alphabet_only_tokens = []\n",
    "#         for token in tokens:\n",
    "#             new_token = ''\n",
    "#             for char in token:\n",
    "#                 # Remove numerical and symbols\n",
    "#                 if char in inclusion:\n",
    "#                     new_token += char\n",
    "#             # Remove stopwords\n",
    "#             if new_token != '' and new_token not in stop_words:\n",
    "#                 alphabet_only_tokens.append(new_token)\n",
    "#         stem_tokens = [stemmer.stem(token).lower() for token in alphabet_only_tokens]\n",
    "#         return stem_tokens\n",
    "\n",
    "    def add_to_dictionary(self, dictionary, term):\n",
    "        dictionary[term] = []\n",
    "        return dictionary[term]\n",
    "\n",
    "    def get_postings_list(self, dictionary, term):\n",
    "        return dictionary[term]\n",
    "\n",
    "    def add_to_postings_list(self, postings_list, doc_id):\n",
    "        postings_list.append(doc_id)\n",
    "\n",
    "    def spimi_invert(self, root, block_size=100000):\n",
    "        files_in_dir = self.directory_listing(root)\n",
    "        block_num = 0\n",
    "        # 1 block is 512 bytes (tracked by sys.getsizeof)\n",
    "        max_byte = block_size * 512\n",
    "        current_size = 0\n",
    "        self.previous_dictionary = {}\n",
    "        self.dictionary = {}\n",
    "        self.start_time = time.time()\n",
    "        for doc_id in (files_in_dir):\n",
    "            file_content = self.file_reading(doc_id)\n",
    "            tokens = tokenizer(self.linguistic_transform(file_content))\n",
    "            for token in tokens:\n",
    "                if token not in self.dictionary:\n",
    "                    current_size += sys.getsizeof(token)\n",
    "                    postings_list = self.add_to_dictionary(self.dictionary, token)\n",
    "                else:\n",
    "                    postings_list = self.get_postings_list(self.dictionary, token)\n",
    "                if current_size + sys.getsizeof(doc_id) > max_byte:\n",
    "                    self.write_block(self.dictionary, block_num)\n",
    "                    self.block_time(block_num, current_size / 512)\n",
    "                    block_num += 1\n",
    "                    self.dictionary = {}\n",
    "                    current_size = 0\n",
    "                    current_size += sys.getsizeof(token)\n",
    "                    postings_list = self.add_to_dictionary(self.dictionary, token)\n",
    "                    current_size += sys.getsizeof(doc_id)\n",
    "                    self.add_to_postings_list(postings_list, doc_id)\n",
    "                else:\n",
    "                    current_size += sys.getsizeof(doc_id)\n",
    "                    self.add_to_postings_list(postings_list, doc_id)\n",
    "        if bool(self.dictionary):\n",
    "            self.write_block(self.dictionary, block_num)\n",
    "            self.block_time(block_num, current_size/512)\n",
    "\n",
    "\n",
    "    def block_time(self, block_num, block_size):\n",
    "        end_time = time.time()\n",
    "        print(f'Time take for BLOCK {block_num}: {end_time-self.start_time}, BLOCK {block_num} size: {block_size}')\n",
    "        self.start_time = time.time()\n",
    "\n",
    "\n",
    "    def write_block(self, dictionary, block_num):\n",
    "        sorted_terms = [term for term in sorted(dictionary)]\n",
    "        output_block_file = os.path.join(self.output_dir, f'BLOCK{block_num}.txt')\n",
    "        with open(output_block_file, 'w', encoding='utf8') as f:\n",
    "            for term in sorted_terms:\n",
    "                line = f'{term} {\" \".join([str(doc_id) for doc_id in dictionary[term]])}\\n'\n",
    "                f.write(line)\n",
    "        self.block_files.append(output_block_file)\n",
    "        return output_block_file\n",
    "\n",
    "\n",
    "    def spimi_merge(self, output_file):\n",
    "        opened_block_files = [open(block_file, encoding='utf-8') for block_file in self.block_files]\n",
    "        file_lines = [block_file.readline()[:-1] for block_file in opened_block_files]\n",
    "        prev_term = ''\n",
    "        first_line = True\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            while (len(opened_block_files)) > 0:\n",
    "                first_index = file_lines.index(min(file_lines))\n",
    "                line = file_lines[first_index]\n",
    "                curr_term = line.split()[0]\n",
    "                curr_postings = ' '.join(line.split()[1:])\n",
    "\n",
    "                if curr_term != prev_term:\n",
    "                    if first_line:\n",
    "                        f.write(f'{curr_term} {curr_postings}')\n",
    "                        first_line = False\n",
    "                    else:\n",
    "                        f.write(f'\\n{curr_term} {curr_postings}')\n",
    "                    prev_term = curr_term\n",
    "                else:\n",
    "                    f.write(f' {curr_postings}')\n",
    "\n",
    "                file_lines[first_index] = opened_block_files[first_index].readline()[:-1]\n",
    "\n",
    "                if file_lines[first_index] == '':\n",
    "                    opened_block_files[first_index].close()\n",
    "                    opened_block_files.pop(first_index)\n",
    "                    file_lines.pop(first_index)\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def check_if_in_str_dict(self, dictionary, pointers, token):\n",
    "        for i in range(len(pointers)-1):\n",
    "            dict_token = dictionary[pointers[i]:pointers[i + 1]]\n",
    "            if dict_token == token:\n",
    "                return i + 1\n",
    "        return False\n",
    "\n",
    "    \n",
    "    \n",
    "    def write_compressed_block(self, dictionary, pointers, posting_list, block_num):\n",
    "        terms = [dictionary[pointers[i]:pointers[i+1]-1] for i in range(len(pointers)-1)]\n",
    "        sorted_terms = sorted(terms)\n",
    "        output_block_file = os.path.join(self.output_dir, f'BLOCK{block_num}.txt')\n",
    "        with open(output_block_file, 'w', encoding='utf8') as f:\n",
    "            for i in range(len(sorted_terms)):\n",
    "                term_index = terms.index(sorted_terms[i])\n",
    "                line = f'{sorted_terms[i]} {\" \".join([str(doc_id) for doc_id in posting_list[term_index]])}\\n'\n",
    "                f.write(line)\n",
    "        self.block_files.append(output_block_file)\n",
    "        return output_block_file\n",
    "        \n",
    "    def spimi_invert_dict_as_str(self, root, block_size=100000):\n",
    "        print('Invert with Compression')\n",
    "        files_in_dir = self.directory_listing(root)\n",
    "        block_num = 0\n",
    "        # 1 block is 512 bytes (tracked by sys.getsizeof)\n",
    "        max_byte = block_size * 512\n",
    "        current_size = 0\n",
    "        self.start_time = time.time()\n",
    "        self.dictionary = '|'\n",
    "        self.pointers = []\n",
    "        self.posting_lists = []\n",
    "\n",
    "        for doc_id in (files_in_dir):\n",
    "            file_content = self.file_reading(doc_id)\n",
    "            tokens = self.linguistic_transform(self.tokenizer(file_content))\n",
    "            for token in tokens:\n",
    "                if f'|{token}|' not in self.dictionary:\n",
    "                    current_size += sys.getsizeof(token)\n",
    "                    self.pointers.append(len(self.dictionary))\n",
    "                    self.dictionary += f'{token}|'\n",
    "                    self.posting_lists.append([])\n",
    "                    postings_list = self.posting_lists[-1]\n",
    "                else:\n",
    "                    pos_in_dict = self.pointers.index(self.dictionary.index(f'|{token}|')+1)\n",
    "                    postings_list = self.posting_lists[pos_in_dict-1]\n",
    "\n",
    "                if current_size + sys.getsizeof(doc_id) > max_byte:\n",
    "                    self.write_compressed_block(self.dictionary, self.pointers, self.posting_lists, block_num)\n",
    "                    self.block_time(block_num, current_size/512)\n",
    "                    block_num += 1\n",
    "                    self.dictionary = '|'\n",
    "                    self.pointers = []\n",
    "                    self.posting_lists = []\n",
    "                    current_size = 0\n",
    "                    current_size += sys.getsizeof(token)\n",
    "                    self.pointers.append(len(self.dictionary))\n",
    "                    self.dictionary += token\n",
    "                    self.posting_lists.append([])\n",
    "                    postings_list = self.posting_lists[-1]\n",
    "                    current_size += sys.getsizeof(doc_id)\n",
    "                    postings_list.append(doc_id)\n",
    "                else:\n",
    "                    current_size += sys.getsizeof(doc_id)\n",
    "                    postings_list.append(doc_id)\n",
    "        if bool(self.dictionary):\n",
    "            self.write_compressed_block(self.dictionary, self.pointers, self.posting_lists, block_num)\n",
    "            self.block_time(block_num, current_size/512)\n",
    "\n",
    "    def spimi_index(self, root, output_file, block_size=100000, compression=None):\n",
    "        index_start_time = time.time()\n",
    "        print('Starting Invert Function')\n",
    "        if compression == None:\n",
    "            self.spimi_invert(root, block_size)\n",
    "        elif compression == 'dict-as-str':\n",
    "            self.spimi_invert_dict_as_str(root, block_size)\n",
    "        merge_start_invert_end_time = time.time()\n",
    "        print(f'Invert Complete, Time Taken: {merge_start_invert_end_time-index_start_time}')\n",
    "        print(f'Starting Merge of {len(self.block_files)} BLOCK files')\n",
    "        self.spimi_merge(output_file)\n",
    "        index_end_time = time.time()\n",
    "        print(f'Merge Complete, Time Taken: {index_end_time-merge_start_invert_end_time}')\n",
    "        print(f'Indexing Completed, Find index file at {output_file}, Time Taken: {index_end_time-index_start_time}')\n",
    "        \n",
    "\n",
    "parser = argparse.ArgumentParser(description='Single-Pass In-Memory Indexing')\n",
    "parser.add_argument('--path', default='../books/')\n",
    "parser.add_argument('--block_size', type=int, default=100000)\n",
    "parser.add_argument('--output', default='output/')\n",
    "parser.add_argument('--filename', default='index.txt')\n",
    "parser.add_argument('--compression', default=None)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# if __name__ == '__main__':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spimi = SPIMI(args.output)\n",
    "spimi.spimi_index(args.path, f'{os.path.join(args.output, args.filename)}', args.block_size, args.compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
